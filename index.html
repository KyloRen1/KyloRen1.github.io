---
layout: standard
title: Bogdan Ivanyuk-Skulskyi
---

<nav class="table-of-contents">
  <ul>
    <li><a href="#about" class="toc-link active" data-section="about">About</a></li>
    <li><a href="#publications" class="toc-link" data-section="publications">Publications</a></li>
    <li><a href="#projects" class="toc-link" data-section="projects">Projects</a></li>
  </ul>
</nav>

<div id="about" class="intro-section">
  <h1 class="title">Hi there, I'm Bogdan</h1>

  <img class="photo" src="/assets/profile.png" alt="Bogdan photo" />

  <div class="content-col">
    <div class="about-intro">
      <p class="lead-text">
        I am a Senior Machine Learning Engineer and <a href="https://www.kaggle.com/bogdanbaraban">Kaggle Expert</a> based in Paris. My work centers on high-scale architecture, bridging the gap between Applied Mathematics and production-grade AI. Currently, I am working on a conversational search agent at Pitchbook Data that supports over 100,000 monthly user interactions.
      </p>
    </div>

    <div class="about-section">
      <p style="margin-bottom: 20px;">
        My professional focus is on building systems that are both mathematically sound and computationally efficient. At Pitchbook, I manage the taxonomy and orchestration for a complex conversational search agent. This role builds on my experience at VINCI (Cyclope.ai), where I engineered real-time computer vision services capable of processing 300 million daily requests with sub-200ms latency.
      </p>
      <p style="margin-bottom: 20px;">
        I hold an MSc in Applied Mathematics from the National University of Kyiv-Mohyla Academy. During my Master's program, I had an internship at Samsung Research, specifically within the Intelligent Security Lab, focusing on voice security applications.
      </p>
      <p style="margin-bottom: 20px;">
        I completed a research residency at the University of Toronto, where I worked under the supervision of <a href="https://kite-uhn.com/scientist/brokoslaw-laschowski">Dr. Brokoslaw Laschowski</a> and <a href="https://ot.utoronto.ca/about/core-faculty/alex-mihailidis/">Dr. Alex Mihailidis</a> on video classification models for exoskeleton control, resulting in three publications and state-of-the-art accuracy on the StairNet dataset.
      </p>
      <p style="margin-bottom: 20px;">
        Most recently, I collaborated with <a href="https://metamobility.cmu.edu/people">Dr. Inseung Kang</a> from Carnegie Mellon University on computer vision-based transfer learning for real-time joint kinematics estimation using smartphone cameras with minimal data, enabling accurate exoskeleton control across novel users in clinical populations.
      </p>
      <p>
        Outside of engineering, I am a robotics enthusiast. I spend my free time building FPV drones and miniature models, or walking my Beagle and Dachshund.
      </p>
    </div>
  </div>
</div>

<div id="publications" class="publications-section">
  <h2 class="section-title">Publications</h2>
    
  <div class="publications-container">
      <!-- 2025 Publications -->
      <div class="pub-card">
        <div class="pub-image-container">
          <img src="/assets/papers/mmlab_2025.png" alt="ICORR 2025" />
        </div>
        <div class="pub-content">
          <span class="pub-year">2025</span>
          <h4 class="pub-title">Personalization of Wearable Sensor-Based Joint Kinematic Estimation Using Computer Vision for Hip Exoskeleton Applications</h4>
          <p class="pub-venue">International Conference On Rehabilitation Robotics (ICORR) 2025</p>
          <p class="pub-authors">Changseob Song, <u>B Ivaniuk-Skulskyi</u>, Adrian Krieger, Kaitao Luo, Inseung Kang</p>
          <p class="pub-description">Computer vision-based deep learning framework enables real-time joint kinematics estimation from smartphone cameras with minimal data (1-2 gait cycles). Transfer learning adaptation reduces error by 9.7-19.9% and demonstrates potential for personalized exoskeleton control across novel users in clinical populations.</p>
          <div class="pub-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/11063180" class="pub-link">Paper</a>
            <span class="pub-link-divider">•</span>
            <a href="https://arxiv.org/pdf/2411.15366" class="pub-link">Pre-print</a>
          </div>
        </div>
      </div>

      <!-- 2024 Publications -->
      <div class="pub-card">
        <div class="pub-image-container">
          <img src="/assets/papers/biorod_2024.png" alt="BioRob 2024" />
        </div>
        <div class="pub-content">
          <span class="pub-year">2024</span>
          <h4 class="pub-title">Sequential Image Classification of Human-Robot Walking Environments using Temporal Neural Networks</h4>
          <p class="pub-venue">IEEE BioRob 2024</p>
          <p class="pub-authors"><u>B Ivaniuk-Skulskyi</u>, A G Kurbis, A Mihailidis, B Laschowski</p>
          <p class="pub-description">State-of-the-art state estimation model on StairNet dataset for walking environment recognition in robotic prosthetic legs and exoskeletons.</p>
          <div class="pub-links">
            <a href="https://ieeexplore.ieee.org/document/10719798" class="pub-link">Paper</a>
            <span class="pub-link-divider">•</span>
            <a href="https://www.biorxiv.org/content/10.1101/2023.11.10.566555v1.full.pdf" class="pub-link">Pre-print</a>
            <span class="pub-link-divider">•</span>
            <a href="https://github.com/KyloRen1/Sequence-Classification-of-Human-Robot-Walking" class="pub-link">Code</a>
          </div>
        </div>
      </div>

      <div class="pub-card">
        <div class="pub-image-container">
          <img src="/assets/papers/Ivanyuk-Skulsky_ACDSA2024.png" alt="ACDSA 2024" />
        </div>
        <div class="pub-content">
          <span class="pub-year">2024</span>
          <h4 class="pub-title">Towards Lightweight Transformer Architecture: an Analysis on Semantic Segmentation</h4>
          <p class="pub-venue">ACDSA 2024 — Oral Presentation</p>
          <p class="pub-authors"><u>B Ivaniuk-Skulskyi</u>, N Shvai, A Llanza, A Nakib</p>
          <p class="pub-description">Novel skip-attention and pool-unpool mechanisms improved transformer inference speed by up to 101.7% on Cityscapes while maintaining segmentation quality.</p>
          <div class="pub-links">
            <a href="https://ieeexplore.ieee.org/document/10467926" class="pub-link">Paper</a>
          </div>
        </div>
      </div>

      <div class="pub-card">
        <div class="pub-image-container">
          <img src="/assets/papers/biomed_eng.png" alt="BioMed Eng" />
        </div>
        <div class="pub-content">
          <span class="pub-year">2024</span>
          <h4 class="pub-title">StairNet: Visual recognition of stairs for human-robot locomotion</h4>
          <p class="pub-venue">BioMedical Engineering OnLine</p>
          <p class="pub-authors">A G Kurbis, D Kuzmenko, <u>B Ivaniuk-Skulskyi</u>, A Mihailidis, B Laschowski</p>
          <p class="pub-description">Comprehensive comparison of state estimation models for walking environment recognition in robotic prosthetics and exoskeletons.</p>
          <div class="pub-links">
            <a href="https://link.springer.com/article/10.1186/s12938-024-01216-0" class="pub-link">Paper</a>
          </div>
        </div>
      </div>

      <div class="pub-card">
        <div class="pub-image-container">
          <img src="/assets/papers/Ivaniuk-Skulsky_ICAIR2023.png" alt="ICRA 2023" />
        </div>
        <div class="pub-content">
          <span class="pub-year">2023</span>
          <h4 class="pub-title">Sequential Image Classification of Human-Robot Walking Environments using Temporal Neural Networks</h4>
          <p class="pub-venue">IEEE ICRA 2023 — Computer Vision for Wearable Robotics Workshop</p>
          <p class="pub-authors"><u>B Ivaniuk-Skulskyi</u>, A G Kurbis, A Mihailidis, B Laschowski</p>
          <p class="pub-description">Temporal neural network approach for dynamic walking environment state estimation in robotic prosthetics.</p>
          <div class="pub-links">
            <a href="/assets/papers/Ivanyuk-Skulsky_ICRA2023.pdf" class="pub-link">Abstract</a>
            <span class="pub-link-divider">•</span>
            <a href="https://www.biorxiv.org/content/10.1101/2023.11.10.566555v1.full.pdf" class="pub-link">Pre-print</a>
            <span class="pub-link-divider">•</span>
            <a href="/assets/papers/Ivaniuk-Skulsky_ICAIR2023.pdf" class="pub-link">Poster</a>
            <span class="pub-link-divider">•</span>
            <a href="https://github.com/KyloRen1/Sequence-Classification-of-Human-Robot-Walking" class="pub-link">Code</a>
          </div>
        </div>
      </div>

      <div class="pub-card">
        <div class="pub-image-container">
          <img src="/assets/papers/generated_images.png" alt="DSMP 2020" />
        </div>
        <div class="pub-content">
          <span class="pub-year">2020</span>
          <h4 class="pub-title">Geometric Properties of Adversarial Images</h4>
          <p class="pub-venue">IEEE DSMP 2020</p>
          <p class="pub-authors"><u>B Ivaniuk-Skulskyi</u>, G Kriukova, A Dmytryshyn</p>
          <p class="pub-description">Linear algebra-based detection method for adversarial perturbations in images with theoretical foundations and experimental validation.</p>
          <div class="pub-links">
            <a href="https://ieeexplore.ieee.org/document/9204251" class="pub-link">Paper</a>
            <span class="pub-link-divider">•</span>
            <a href="https://github.com/KyloRen1/Geometric-properties-of-adversarial-images" class="pub-link">Code</a>
          </div>
        </div>
      </div>
    </div>

</div>

<div id="projects" class="projects-section">
  <h2 class="section-title">Selected Projects</h2>
  
  <ul class="projects-list">
    <li>
      <h3>UA-Datasets</h3>
      <p>Comprehensive collection of datasets for the Ukrainian language. Available as a Python package and on Hugging Face Hub.</p>
      <div class="project-link-group">
        <a href="https://fido-ai.github.io/ua-datasets/">Documentation</a>
        <a href="https://github.com/fido-ai/ua-datasets">GitHub</a>
        <a href="https://huggingface.co/FIdo-AI">Hugging Face</a>
      </div>
    </li>

    <li>
      <h3>Tower Defence RL Agent</h3>
      <p>Implementation of a classical Tower Defence game with a gym-like environment for training reinforcement learning agents.</p>
      <img class="project-image" src="/assets/projects/tower_defence.gif" alt="Tower Defence RL agent" />
      <a href="https://github.com/KyloRen1/TowerDefenceRLagent">View on GitHub →</a>
    </li>

    <li>
      <h3>Raspberry Pi Car</h3>
      <p>Building a Raspberry Pi-powered autonomous car from scratch, including hardware assembly and remote control features.</p>
      <div class="project-images-grid">
        <img src="/assets/projects/rasp_pi/car.gif" alt="Raspberry Pi car" />
        <img src="/assets/projects/rasp_pi/test.gif" alt="Raspberry Pi testing" />
      </div>
      <a href="https://github.com/KyloRen1/RaspberryPi-car">View on GitHub →</a>
    </li>

    <li>
      <h3>Surviv.io Game Bot</h3>
      <p>Reinforcement learning agent for the online multiplayer battle royale game surviv.io.</p>
      <img class="project-image" src="/assets/projects/surviv.gif" alt="Surviv.io RL agent" />
      <a href="https://github.com/Laggg/ml-bots-surviv.io">View on GitHub →</a>
    </li>

    <li>
      <h3>Quora Insincere Questions Classification</h3>
      <p>Kaggle NLP competition solution achieving silver medal in top 2% of competitors.</p>
      <a href="https://github.com/KyloRen1/Kaggle-Quora-Insincere-Questions-Classification">View on GitHub →</a>
    </li>

    <li>
      <h3>Cross-lingual Multilingual BERT Analysis</h3>
      <p>Research on M-BERT's transfer learning capabilities across languages, comparing English-to-Russian transfer with RuBERT.</p>
      <a href="https://github.com/KyloRen1/CS224N-NLP-DeepPavlov">View on GitHub →</a>
    </li>
  </ul>

  <div class="contact-section">
    <p>Let's <a href="mailto:ivanyuk.bogdan1999@gmail.com">get in touch!</a></p>
  </div>
</div>

<script src="{{ '/assets/scroll-nav.js' | relative_url }}"></script>
