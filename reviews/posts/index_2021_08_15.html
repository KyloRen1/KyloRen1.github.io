---
layout: standard
title: Bogdan Ivanyuk-Skulskyi - Reviews
---

<div>
    <h1><span style="font-size: 0.85em; font-weight: bold;">Paper review: Do Wider Neural Networks Really Help Adversarial Robustness?</span></h1>
    <p><span style="background-color: #f2f2f2; padding: 2px 4px; border-radius: 4px; font-weight: bold;">Date</span>: 2021-08-14</p>
    <p><span style="background-color: #f2f2f2; padding: 2px 4px; border-radius: 4px; font-weight: bold;">Tags</span>: Wide ResNet, robust classifier, Lipschitz continuity</p>

    <p>The author's associated resistance to matrix perturbations with the local Lipschitz characteristic of the 
        neural network. It follows from this theory that broader models (such as WideResNet) are less stable to matrix 
        perturbations than narrower ones. To remedy the situation, it is worth increasing the stability parameter 
        <em>位</em> in the process of adversarial training. Also, they showed the results of the dependence of the 
        network width and the stability parameter <em>位</em> on the CIFAR10 and CIFAR100 datasets.</p>

    <h2>Adversarial Training</h2>
    <p>A form of data augmentation, in which a dataset of adversarial samples is first collected 
        (samples whose distance from real images does not go beyond the specified value <em></em> in <em></em> norm), 
        and then the model is trained on a combination of original and adversarial data as in the formula below:</p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/adversarial_training.png" alt=""/></p>
    
    <br /><br />
    <h2>Empirical Research</h2>
    <p><img align="right" src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/adversarial_training_framework.png" alt="drawing" width="350"/></p>
    <p><strong>Robust accuracy</strong> is the main metric for assessing network stability, which measures the ratio of correctly classified samples after an attack.<br />
    <strong>Natural accuracy</strong> - the usual accuracy on samples before the attack.<br />
    <strong>Perturbation stability</strong> is a new metric that measures the percentage of samples whose labels do not change after an attack.</p>
    <p>As can be seen in the graph (Figure 3), the metric of resilience to changes decreases monotonically with increasing model width. This suggests that broader models are actually more vulnerable to matrix changes.</p>
    <br />
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/metric_plots.png" alt="drawing" width="2000"/></p>
    
    <br />
    <h2>Local Lipschitzness and Model Width</h2>
    <p><img align="right" src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/lipschitz_empirical_calculations_1.png" alt="drawing" width="350"/></p>
    <p>Many authors associate local Lipschitzness (Lemma 4.1) with network stability to matrix perturbations, suggesting that lower local Lipschitzness leads to more robust models.</p>
    <p>In this article, the authors have shown that local Lipschitzness increases with increasing model width (Figure 4). More precisely, the local Lipschitzness is scaled as the square root of the model width (Lemma 4.2).</p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/lipschitz_continuity.png" alt=""/></p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/lipschitz_constant.png" alt=""/></p>

    <h2>Experiments</h2>
    <p>It is known from past sections that wider models are less resistant to perturbation stability. One of the ways to improve stability is to increase the parameter that is responsible for network stability <em>位</em>.</p>
    <p>The main experiments were carried out on the CIFAR10 dataset using WideResNet with different widths (1, 5, 10). The batch size is 128, the number of epochs is 100. LR is 0.1 and is halved at each epoch, after the 75th epoch.</p>
    <p>The best robust accuracy for a model of width 1 is obtained using <em>位</em> = 6. For width 5: <em>位</em> = 12; for 10: <em>位</em> = 18.</p>
    <p>The results are summarized in Tables 1 and 2.</p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/table1.png" alt=""/></p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/table2.png" alt=""/></p>
        
    <hr>
    <br />
    <p><span style="background-color: #f2f2f2; padding: 2px 4px; border-radius: 4px; font-weight: bold;">Paper</span>: 
        <a href="https://arxiv.org/pdf/2010.01279.pdf">https://arxiv.org/pdf/2010.01279.pdf</a></p>
   </div>
