---
layout: standard
title: Bogdan Ivanyuk-Skulskyi - Reviews
---

<div class="review-post">
  <a href="/reviews" class="back-to-reviews">Back to Reviews</a>
  
  <header class="review-post-header">
    <h1 class="review-post-title">Do Wider Neural Networks Really Help Adversarial Robustness?</h1>
    <div class="review-meta">
      <div class="review-meta-item">
        <span class="meta-label">Date</span>
        <span>August 14, 2021</span>
      </div>
      <div class="review-tags">
        <span class="review-tag">Wide ResNet</span>
        <span class="review-tag">robust classifier</span>
        <span class="review-tag">Lipschitz continuity</span>
      </div>
    </div>
  </header>

  <div class="review-content">
    <p>The author's associated resistance to matrix perturbations with the local Lipschitz characteristic of the 
        neural network. It follows from this theory that broader models (such as WideResNet) are less stable to matrix 
        perturbations than narrower ones. To remedy the situation, it is worth increasing the stability parameter 
        <em>λ</em> in the process of adversarial training. Also, they showed the results of the dependence of the 
        network width and the stability parameter <em>λ</em> on the CIFAR10 and CIFAR100 datasets.</p>

    <h2>Adversarial Training</h2>
    <p>A form of data augmentation, in which a dataset of adversarial samples is first collected 
        (samples whose distance from real images does not go beyond the specified value <em>&epsilon;</em> in 
        <em>L<sub>p</sub></em> norm), and then the model is trained on a combination of original and adversarial 
        data as in the formula below:</p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/adversarial_training.png" alt="Adversarial Training Formula"/></p>
    
    <h2>Empirical Research</h2>
    <p><img align="right" src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/adversarial_training_framework.png" alt="Adversarial Training Framework" width="350"/></p>
    <p><strong>Robust accuracy</strong> is the main metric for assessing network stability, which measures the ratio of correctly classified samples after an attack.<br />
    <strong>Natural accuracy</strong> - the usual accuracy on samples before the attack.<br />
    <strong>Perturbation stability</strong> is a new metric that measures the percentage of samples whose labels do not change after an attack.</p>
    <p>As can be seen in the graph (Figure 3), the metric of resilience to changes decreases monotonically with increasing 
        model width. This suggests that broader models are actually more vulnerable to matrix changes.</p>
    <br />
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/metric_plots.png\" alt="Metric Plots" width="2000"/></p>
    
    <h2>Local Lipschitzness and Model Width</h2>
    <p><img align="right" src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/lipschitz_empirical_calculations_1.png" alt="Lipschitz Calculations" width="350"/></p>
    <p>Many authors associate local Lipschitzness (Lemma 4.1) with network stability to matrix perturbations, suggesting that lower local Lipschitzness leads to more robust models.</p>
    <p>In this article, the authors have shown that local Lipschitzness increases with increasing model width (Figure 4). More precisely, the local Lipschitzness is scaled as the square root of the model width (Lemma 4.2).</p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/lipschitz_continuity.png" alt="Lipschitz Continuity"/></p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/lipschitz_constant.png" alt="Lipschitz Constant"/></p>

    <h2>Experiments</h2>
    <p>It is known from past sections that wider models are less resistant to perturbation stability. One of the ways to improve stability is to increase the parameter that is responsible for network stability <em>λ</em>.</p>
    <p>The main experiments were carried out on the CIFAR10 dataset using WideResNet with different widths (1, 5, 10). The batch size is 128, the number of epochs is 100. LR is 0.1 and is halved at each epoch, after the 75th epoch.</p>
    <p>The best robust accuracy for a model of width 1 is obtained using <em>λ</em> = 6. For width 5: <em>λ</em> = 12; for 10: <em>λ</em> = 18.</p>
    <p>The results are summarized in Tables 1 and 2.</p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/table1.png" alt="Results Table 1"/></p>
    <p><img src="/assets/reviews/Do_wider_neural_networks_really_help_adversarial_robustness/table2.png" alt="Results Table 2"/></p>
  </div>

  <div class="review-footer">
    <h3 class="review-footer-title">Resources</h3>
    <div class="review-links">
      <div class="review-link-item">
        <span class="link-label">Paper:</span>
        <a href="https://arxiv.org/pdf/2010.01279.pdf">https://arxiv.org/pdf/2010.01279.pdf</a>
      </div>
    </div>
  </div>
</div>
