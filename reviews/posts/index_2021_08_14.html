---
layout: standard
title: Bogdan Ivanyuk-Skulskyi - Reviews
---

<div class="review-post">
  <a href="/reviews" class="back-to-reviews">Back to Reviews</a>
  
  <header class="review-post-header">
    <h1 class="review-post-title">A Closer Look at Accuracy vs. Robustness</h1>
    <div class="review-meta">
      <div class="review-meta-item">
        <span class="meta-label">Date</span>
        <span>August 2, 2021</span>
      </div>
      <div class="review-tags">
        <span class="review-tag">adversarial robustness</span>
        <span class="review-tag">robust classifier</span>
        <span class="review-tag">Lipschitz continuity</span>
        <span class="review-tag">dropout</span>
      </div>
    </div>
  </header>

  <div class="review-content">
    <p>Modern methods for training robust neural networks lead to a deterioration in the accuracy of test data. 
        Therefore, past research has assumed that a trade-off between reliability and accuracy may be inevitable.</p>

    <p>This paper shows that most datasets are <i>r</i>-separable - examples from different classes are at least <i>2r</i> 
        apart in pixel space. This <i>r</i>-division is valid for <i>r</i> values that exceed the radius of the matrix 
        perturbations <i>&epsilon;</i>. Therefore, there is a classifier that is both accurate and robust to matrix perturbations 
        in the size of <i>r</i>.</p>
    <p align="center">
        <img src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/table1.png" alt="Dataset Separability" width="400"/>
    </p>

    <h2>R-separability of Datasets</h2>
    <p>The article covers the MNIST, CIFAR-10, SVHN, and Restricted ImageNet datasets. All of them have separability greater 
        than <i>2&epsilon;</i>. Table 1 shows statistics of the distances between samples from the training data and their 
        nearest neighbors from other classes in the <i>L<sub>inf</sub></i> norm. Figure 2 shows histograms of the separability 
        of the train and test data.</p>

    <p><img src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/figure2.png" alt="Separability Histograms"/></p>

    <h2>Accuracy and Stability to Matrix Perturbations for r-separable Data</h2>
    <p>It is theoretically proven that with <i>r</i>-separability of data, there exists a classifier based on a locally 
        Lipschitz function, which has a robust accuracy 1 with radius <i>r</i>.</p>

    <p><img src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/lemma41.png" alt="Lemma 4.1"/></p>
    <p><img src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/theorem42.png" alt="Theorem 4.2"/></p>
    <p><img src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/lemma43.png" alt="Lemma 4.3"/></p>

    <h2>Existing Methods of Training Models</h2>
    <p><img align="right" src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/figure4.png" alt="Training Methods Comparison" width="350" style="margin-left:5px;margin-right:5px"/></p>
    <p>The experimental part of the article compares different training methods (gradient regularization, competitive training, 
        TRADES, Robust Self Training, and others) that are attacked by <a href="https://arxiv.org/pdf/1611.01236.pdf">projected gradient descent</a> 
        and <a href="https://arxiv.org/pdf/1910.09338.pdf">multipurpose attack</a>. Tables 2 and 3 show basic statistics.</p>

    <p>The tables show that the methods of competitive training (AT), stable self-learning (RST), and TRADES are more stable than others.
         Local Lipschitzness is most correlated with adversarial accuracy. More reliable methods assume a higher degree of local Lipschitzness, 
         but there are cases when the accuracy on test data decreases, although the value of local Lipschitzness continues to decrease.</p>

    <p><img src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/table2.png" alt="Comparison Table 2"/></p>
    <p><img src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/table3.png" alt="Comparison Table 3"/></p>

    <p>It is worth noting that the methods that train locally Lipschitz classifiers have generalization gaps: a large difference 
        in accuracy on training and test data, and even more for training and attack-prone test data.
    The authors managed to reduce the difference in accuracy by adding a dropout. This combination of dropout and stable training 
    methods improves conventional accuracy, adversarial accuracy, and local Lipschitzness.</p>

    <p><img src="/assets/reviews/A_closer_look_at_accuracy_vs_robustness/table4.png" alt="Dropout Results"/></p>
  </div>

  <div class="review-footer">
    <h3 class="review-footer-title">Resources</h3>
    <div class="review-links">
      <div class="review-link-item">
        <span class="link-label">Paper:</span>
        <a href="https://arxiv.org/pdf/2003.02460.pdf">https://arxiv.org/pdf/2003.02460.pdf</a>
      </div>
      <div class="review-link-item">
        <span class="link-label">Code:</span>
        <a href="https://github.com/yangarbiter/robust-local-lipschitz">https://github.com/yangarbiter/robust-local-lipschitz</a>
      </div>
    </div>
  </div>
</div>
