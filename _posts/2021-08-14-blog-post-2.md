---
title: 'Paper review: A Closer Look at Accuracy vs. Robustness'
date: 2021-08-02
permalink: /posts/2021/08/blog-post-2/
tags:
  - adversarial robustness
  - robust classifier
  - Lipschitz continuity
  - dropout
---

Modern methods for training robust neural networks lead to a deterioration in the accuracy of test data. Therefore, past research has assumed that a trade-off between reliability and accuracy may be inevitable.

This paper shows that most datasets are $$r$$-separable - examples from different classes are at least $$2r$$ apart in pixel space. This $$r$$-division is valid for $$r$$ values that exceed the radius of the matrix perturbations $$ùõÜ$$. Therefore, there is a classifier that is both accurate and robust to matrix perturbations in the size of $$r$$.
<br />
<p align="center">
<img src="/images/blog_images/A_closer_look_at_accuracy_vs_robustness/table1.png" alt="drawing" width="400"/>
</p>

R-separability of datasets
------
The article covers the MNIST, CIFAR-10, SVHN, and Restricted ImageNet datasets. All of them have separability greater than $$2ùõÜ$$. Table 1 shows statistics of the distances between samples from the training data and their nearest neighbors from other classes in the $$L_{inf}$$ norm. Figure 2 shows histograms of the separability of the train and test data.

![](/images/blog_images/A_closer_look_at_accuracy_vs_robustness/figure2.png)


Accuracy and stability to matrix perturbations for r-separable data
------
It is theoretically proven that with $$r$$-separability of data, there exists a classifier based on a locally Lipschitz function, which has a robust accuracy 1 with radius $$r$$.

![](/images/blog_images/A_closer_look_at_accuracy_vs_robustness/lemma41.png)

![](/images/blog_images/A_closer_look_at_accuracy_vs_robustness/theorem42.png)

![](/images/blog_images/A_closer_look_at_accuracy_vs_robustness/lemma43.png)


Existing methods of training models
------
<img align="right" src="/images/blog_images/A_closer_look_at_accuracy_vs_robustness/figure4.png" alt="drawing" width="350" style="margin-left:5px;margin-right:5px"/>
The experimental part of the article compares different training methods (gradient regularization, competitive training, TRADES, Robust Self Training, and others) that are attacked by [projected gradient descent](https://arxiv.org/pdf/1611.01236.pdf) and [multipurpose attack](https://arxiv.org/pdf/1910.09338.pdf). Tables 2 and 3 show basic statistics.

The tables show that the methods of competitive training (AT), stable self-learning (RST), and TRADES are more stable than others. Local Lipschitzness is most correlated with adversarial accuracy. More reliable methods assume a higher degree of local Lipschitzness, but there are cases when the accuracy on test data decreases, although the value of local Lipschitzness continues to decrease.

![](/images/blog_images/A_closer_look_at_accuracy_vs_robustness/table2.png)

![](/images/blog_images/A_closer_look_at_accuracy_vs_robustness/table3.png)

It is worth noting that the methods that train locally Lipschitz classifiers have generalization gaps: a large difference in accuracy on training and test data, and even more for training and attack-prone test data.
The authors managed to reduce the difference in accuracy by adding a dropout. This combination of dropout and stable training methods improves conventional accuracy, adversarial accuracy, and local Lipschitzness.

![](/images/blog_images/A_closer_look_at_accuracy_vs_robustness/table4.png)

------

__Paper__: [https://arxiv.org/pdf/2003.02460.pdf](https://arxiv.org/pdf/2003.02460.pdf) \
__Code__: [https://github.com/yangarbiter/robust-local-lipschitz](https://github.com/yangarbiter/robust-local-lipschitz)